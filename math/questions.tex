\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{float}
\usepackage{etoolbox}\AtBeginEnvironment{algorithmic}{\small}

%\setlength{\parindent}{4em}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\one}{\mathbf{1}}
\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathOperator*{\argmin}{arg\,min}


\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\algnewcommand{\IfThenElse}[3]{\State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3}

\begin{document}


\title{Pre-interview Task}
\author{William Maxwell} 
\maketitle
\paragraph{Problem 1a.}
The equivalence between minimizing $K$-means and maximizing intra-cluster kernel similarity was shown by Ding, He, and Simon~\cite{Ding2005}. 
We follow their approach. Let $C_1,\dots,C_K$ be the clusters found by $K$-means and $c_1,\dots,c_K$ be their centroids. That is, $c_k = \sum_{i \in C_k} x_i/|C_k|$. We start by rewriting the $K$-means objective function as a sum of inner products.
\begin{align*}
J_{k\text{-means}} &= \sum_{k=1}^K \sum_{i \in C_k} \|x_i - c_k\|^2\\
 &= \sum_{k=1}^K \sum_{i \in C_k} \|x_i\|^2 + \|c_k\|^2 - 2x_i^T c_k \\
&= \sum_{i=1}^n \|x_i\|^2 + \sum_{k=1}^K \sum_{i \in C_k} \|c_k\|^2 - 2 \sum_{k=1}^K \sum_{i \in C_k} x_i^T c_k \\
&= \sum_{i=1}^n \|x_i\|^2 + \sum_{k=1}^K |C_k| \|c_k\|^2 - 2 \sum_{k=1}^K |C_k| \|c_k\|^2 \\
&= \sum_{i=1}^n \|x_i\|^2 - \sum_{k=1}^K |C_k|\|c_k\|^2\\
&= \sum_{i=1}^n \|x_i\|^2 - \sum_{k=1}^K \frac{1}{|C_k|} \sum_{i, j \in C_k} x_i^T x_j
\end{align*}
Since $\sum_{i=1}^n \|x_i\|^2$ is a constant, $J_{K\text{-means}}$ is minimized when $\sum_{k=1}^K \frac{1}{|C_k|} \sum_{i, j \in C_k} x_i^T x_j$ is maximized. Let $H$ be an indicator matrix for the clusters returned by $K$-means. We compute the entries of $HAH^T$ where $A=X^TX$, \[(HAH^T)_{ij} =  \frac{1}{\sqrt{C_i C_j}} \sum_{x \in C_i}\sum_{y \in C_j} x^T y. \] Hence, on the diagonal we have \[(HAH^T)_{ii} =  \frac{1}{|C_k|} \sum_{i, j \in C_k} x_i^T x_j, \] from which it follows that \[ \Tr(HAH^T) = \sum_{k=1}^K \frac{1}{|C_k|} \sum_{i, j \in C_k} x_i^T x_j.\]
Thus, we conclude $\argmin_{H} J_{K\text{-means}} = \argmax_H \Tr(HAH^T)$ where $H$ ranges over all orthogonal indicator matrices.

\paragraph{Problem 1b.}
Now, we can show that $K$-means can be formulated as an NNMF problem under the constraints that $H$ is an orthogonal indicator matrix and $W = H^T$.
\begin{align*}
\argmin_{H} J_{k\text{-means}} &= \argmax_H \Tr(HAH^T) \\
&= \argmin_H -2\Tr(HAH^T) \\
&= \argmin_H \|A\|^2 -2 \Tr(HAH^T) + \|H^TH\|^2 \\
&= \argmin_H \|A - HH^T\|^2
\end{align*}

\paragraph{Problem 2a.}
NNMF was shown to take the form of a PLSA problem by Ding, Li, and Peng~\cite{Ding2008}. We follow their approach.
A PLSA task asks to perform the following joint probability factorization \[p(w_i, d_j) \approx \sum_{k=1}^K p(w_i | k) p(d_j | k) p(k) \] given $p(w_i, d_j)$ and $K$. Consider the matrix $A_{ij} = p(w_i, d_j)$ we will show that factorizing $A \approx WH$ can be viewed as a PSLA task.
First, note the following \[ \sum_{i=1}^m p(w_i|k) = 1, \quad  \sum_{j=1}^n p(d_j|k) = 1, \quad \sum_{k=1}^K p(k) = 1.\]
Let $A=WH$ be some non-negative factorization and define the diagonal matrices $Y_{kk} = \sum_{i=1}^m W_{ik}$, $Z_{kk} = \sum_{j=1}^n H_{kj}$.
It follows that $\tilde{W} = WY^{-1}$ is normalized such that each column has $\ell_1$-norm 1, and $\tilde{H} = Z^{-1}H$ is normalized such that each row has $\ell_1$-norm 1. Moreover, we have $A = WH = \tilde{W} Y Z \tilde{H}$.
We have the equalities \[\sum_{i=1}^m \tilde{W}_{ik} = 1, \quad \sum_{j=1}^n \tilde{H}_{kj} = 1,\] hence we can interpret $\tilde{W}_{ik} = p(w_i|k)$ and $\tilde{H}_{kj} = p(d_j|k)$.
Moreover, we have \[1 = \sum_{i=1}^m \sum_{j=1}^n p(w_i, d_j) = \sum_{i=1}^m \sum_{j=1}^n A_{ij} = \sum_{i=1}^m \sum_{j=1}^n \sum_{k=1}^K \tilde{W}_{ik} \left( YZ \right)_{kk} \tilde{H}_{kj} = \sum_{k=1}^K \left(Y Z \right)_{kk}\] so we can interpret $(YZ)_{kk} = p(k)$.
Thus, the non-negative matrix factorization $A = \tilde{W} ZY \tilde{H}$ induces the PLSA factorization \[p(w_i, d_j) = A_{ij} = \sum_{k=1}^K \tilde{W}_{ik} \left(YZ \right)_{kk} \tilde{H}_{kj} = \sum_{k=1}^K p(w_i|k)p(d_j|k)p(k). \]

\paragraph{Problem 2b.}
Consider an $m \times n$ matrix $A$ with $A_{ij} = p(w_i, d_j)$. Performing $K$-means on $A$ yields an $m \times K$ matrix of centroids $W$, and a $K \times n$ indicator matrix $H$.
Viewing $A \approx WH = \tilde{W}YZ\tilde{H}$ as an approximate non-negative matrix factorization, we interpret the PLSA joint probability factorization. By $c_k$ we denote the centroid of the $k$th cluster, and $c_k(i)$ the $i$th feature of the centroid. We have,
\[p(w_i | k) \approx \tilde{W}_{ik} = \frac{c_k(i)}{\sum_{j=1}^m c_k(j)}.  \]
In other words, $p(w_i | k)$ is the normalized weight of the $i$th feature in the $k$th centroid.
We also have, \[ p(d_j | k) \approx \tilde{H}_{kj} = \begin{cases} \frac{1}{|C_k|} & x_j \in C_k \\ 0 & \text{otherwise} \end{cases}\] which can be interpreted as the probability of sampling the $j$th vector conditioned on the $k$th cluster.
Finally, we have \[p(k) \approx (ZY)_{kk} = \|c_k\|_1 |C_k| = \sum_{i=1}^m \sum_{x_j \in C_k} p(w_i, d_j)\] which is the probability that the event which occurs is in the form $p(w_i, d_j)$ where $x_j$ is in the $k$th cluster.

\paragraph{Problem 2c.}
To drop the indicator vectors we compute $\argmin_H \|A - H^TH\|^2_F $ under the constraints that $H \geq 0$, however we do not require that $HH^T = I$. Now, each column may have more than one non-zero entry. We interpret the entry $H_{ij}$ as a weight that cluster $i$ assigns to the vector $x_j$. Hence, we can compute the centroids as a weighted average $c_k = \frac{1}{n}\sum_{j=1}^n H_{kj} x_j$.

\bibliographystyle{plain}
\bibliography{questions}

\end{document}

